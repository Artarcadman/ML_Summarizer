{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
    "import shutil\n",
    "\n",
    "print(\"Загрузка полного датасета XLSum English...\")\n",
    "train_url = \"https://huggingface.co/api/datasets/csebuetnlp/xlsum/parquet/english/train/0.parquet\"\n",
    "test_url = \"https://huggingface.co/api/datasets/csebuetnlp/xlsum/parquet/english/test/0.parquet\"\n",
    "\n",
    "df_train_full = pd.read_parquet(train_url)\n",
    "df_test_full = pd.read_parquet(test_url)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", legacy=False)\n",
    "\n",
    "# Фильтрация записей. Оставляем только те, которые короче 512 токенов\n",
    "def is_short_enough(text):\n",
    "    # Быстрая проверка: если слов меньше 400, скорее всего влезет (экономим время на токенизации)\n",
    "    if len(str(text).split()) <= 400: return True\n",
    "    # Точная проверка для пограничных случаев\n",
    "    return len(tokenizer.encode(str(text))) <= 512\n",
    "\n",
    "print(\"Фильтрация данных ...\")\n",
    "df_train_filtered = df_train_full[df_train_full['text'].apply(is_short_enough)]\n",
    "df_test_filtered = df_test_full[df_test_full['text'].apply(is_short_enough)]\n",
    "\n",
    "train_data = Dataset.from_pandas(df_train_filtered)\n",
    "test_data = Dataset.from_pandas(df_test_filtered)\n",
    "\n",
    "print(f\"Итого для обучения: {len(train_data)} примеров (отфильтровано {(len(df_train_full)-len(df_train_filtered))} длинных статей)\")\n",
    "\n",
    "# Токенизация датасета\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + str(doc) for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Токенизация отфильтрованных данных...\")\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True, remove_columns=test_data.column_names)\n",
    "\n",
    "# Модель и обучение\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",  \n",
    "    eval_steps=500,             \n",
    "    learning_rate=3e-5,           \n",
    "    per_device_train_batch_size=8, \n",
    "    num_train_epochs=1,           \n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,           \n",
    "    logging_steps=100,\n",
    "    fp16=True,              \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    ")\n",
    "\n",
    "print(\"Запуск масштабного обучения...\")\n",
    "trainer.train()\n",
    "\n",
    "# Сохранение\n",
    "save_path = \"./my_full_model_t5\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "shutil.make_archive(\"full_model_weights\", 'zip', save_path)\n",
    "\n",
    "print(\"Модель готова! 'full_model_weights.zip'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
